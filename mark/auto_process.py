import os
import re
import sys
import json
import shutil
import subprocess
from datetime import datetime
import time
import numpy as np
from supabase import create_client
# from infer_location import infer_location_clip
from pathlib import Path
from infer_location import infer_area_by_kp
import math


SUPABASE_URL = "https://polqjhuklxclnvgpjckf.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InBvbHFqaHVrbHhjbG52Z3BqY2tmIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MjI4MTA5NywiZXhwIjoyMDY3ODU3MDk3fQ.tA_l_KmEsm3YlnPfohlwaYiOG3fnTrbZRlJGUCpkWnk"
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

BASE_CONFIG_DIR = "base_config"
BASE_IMAGES_ROOT = Path(r"E:\ParkSavvy") 
DOWNLOAD_DIR = "downloads"
os.makedirs(BASE_CONFIG_DIR, exist_ok=True)
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# ----------------- å¹¾ä½•å‡½å¼ -----------------
def reorder_box_points(points):
    """æŠŠ 4 é»æ’åºæˆé †æ™‚é‡"""
    pts = np.array(points, dtype=float)
    center = np.mean(pts, axis=0)
    angles = np.arctan2(pts[:,1]-center[1], pts[:,0]-center[0])
    pts = pts[np.argsort(angles)]
    return pts

def align_points_to_centerline(points, box_points, padding_ratio=0.1, smooth_factor=0.3):
    """
    ç´…é»æ²¿è‘—åœè»Šä½ä¸­å¿ƒç·šæ’åˆ—ï¼Œå¸¶åŸå§‹è·é›¢æ¬Šé‡çš„å¹³æ»‘åˆ†å¸ƒ
    """
    box_points = reorder_box_points(box_points)

    # è¨ˆç®—é‚Šé•·
    edges = [(box_points[i], box_points[(i+1)%4]) for i in range(4)]
    lengths = [np.linalg.norm(e[1]-e[0]) for e in edges]

    # æ‰¾æœ€çŸ­é‚Šï¼ˆåœè»Šä½é ­å°¾ï¼‰
    short_idx = int(np.argmin(lengths))
    short_edge = edges[short_idx]
    opp_edge   = edges[(short_idx+2)%4]

    # ä¸­å¿ƒç·š
    center_start = (short_edge[0]+short_edge[1])/2
    center_end   = (opp_edge[0]+opp_edge[1])/2
    dir_vec = center_end - center_start
    dir_len = np.linalg.norm(dir_vec)
    dir_vec /= dir_len

    # 1ï¸âƒ£ åŸå§‹æŠ•å½±è·é›¢
    proj = [np.dot(pt-center_start, dir_vec) for pt in points]
    sorted_idx = np.argsort(proj)
    proj = np.array(proj)[sorted_idx]

    # 2ï¸âƒ£ åŸå§‹è·é›¢æ­£è¦åŒ–
    min_proj, max_proj = proj.min(), proj.max()
    proj_range = max_proj - min_proj if max_proj>min_proj else 1.0
    norm_proj = (proj - min_proj) / proj_range

    # 3ï¸âƒ£ ç·šæ€§ç­‰è· + åŸå§‹è·é›¢æ··åˆ
    n = len(points)
    linear = np.linspace(0, 1, n)
    mixed = (1-smooth_factor)*linear + smooth_factor*norm_proj

    # 4ï¸âƒ£ åŠ  padding
    start_pos = dir_len * padding_ratio
    end_pos   = dir_len * (1 - padding_ratio)
    usable_len = end_pos - start_pos
    aligned_proj = start_pos + mixed * usable_len

    # 5ï¸âƒ£ é‚„åŸç©ºé–“åº§æ¨™
    aligned_points = [center_start + dir_vec*p for p in aligned_proj]
    return np.array(aligned_points)

def pixel_to_latlng(x, y, cfg):
    """åº•åœ–åƒç´ è½‰ç¶“ç·¯åº¦"""
    if cfg["img_width"] == 0 or cfg["img_height"] == 0:
        return None, None
    lng = cfg["lng_min"] + (x / cfg["img_width"]) * (cfg["lng_max"] - cfg["lng_min"])
    lat = cfg["lat_max"] - (y / cfg["img_height"]) * (cfg["lat_max"] - cfg["lat_min"])
    return lat, lng

# ----------------- DB æ“ä½œ -----------------

def get_unprocessed_images_raw(limit=100):
    r = (
        supabase.table("image_uploads")
        .select("id, filename, created_at, inferred_area, processed, location")
        .eq("processed", False)
        .order("created_at", desc=True)
        .limit(limit)
        .execute()
    )
    rows = r.data or []
    # åªç•™åœ–ç‰‡æª”
    return [
        row for row in rows
        if isinstance(row.get("filename"), str)
        and row["filename"].lower().endswith((".jpg", ".jpeg", ".png"))
    ]

def download_image(filename):
    source_path = os.path.join(r"E:\ParkSavvy\uploads", filename)
    save_path = os.path.join(DOWNLOAD_DIR, filename)
    if not os.path.exists(source_path):
        print(f" æ‰¾ä¸åˆ°åœ–ç‰‡ï¼š{source_path}")
        return None
    shutil.copyfile(source_path, save_path)
    print(f" å·²è¤‡è£½åœ–ç‰‡ï¼š{filename}")
    return save_path

def mark_as_processed(image_id):
    supabase.table("image_uploads").update({"processed": True}).eq("id", image_id).execute()

def upload_motor_records(result_path, area_key, filename):
    """æ¸…ç©ºè©²ã€å€éµ(=inferred_area)ã€‘çš„ motor_recordsï¼Œæ’å…¥æœ€æ–°åµæ¸¬çµæœ"""
    with open(result_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # å…ˆæ¸…æ‰åŒå€éµçš„èˆŠè³‡æ–™
    supabase.table("motor_records").delete().eq("location", area_key).execute()
    print(f"ğŸ§¹ å·²æ¸…é™¤ {area_key} å…ˆå‰ç´€éŒ„")

    records = []
    for item in data:
        md = item.get("match_distance")
        if not isinstance(md, (int, float)) or not math.isfinite(md):
            md = None

        records.append({
            "image_filename": filename,
            "motor_index": item["motor_index"],
            "location": area_key,                 # â˜… ç”¨ inferred_areaï¼ˆå¦‚ ib_H01 / tr_A02ï¼‰
            "real_x": item["real_x"],
            "real_y": item["real_y"],
            "plate_text": item["plate_text"],
            "match_distance": md,
            "created_at": datetime.now().isoformat(),
        })

    if records:
        supabase.table("motor_records").insert(records).execute()
        print(f"å·²ä¸Šå‚³ {len(records)} ç­†é…å°è³‡æ–™")
    else:
        print(" æ²’æœ‰é…å°è³‡æ–™å¯ä¸Šå‚³")

def upsert_current_count(route_key: str, area_id: str, count: int, src_id: str | None = None):
    """æŠŠæœ€æ–°æ•¸é‡å¯«é€² current_statusï¼ˆè¤‡åˆä¸»éµ route_key+area_idï¼‰"""
    payload = {
        "route_key": (route_key or "").lower(),  # "ib" / "tr"
        "area_id": area_id,                      # "A01" / "B02"ï¼ˆä¸è¦æœ‰å‰ç¶´ï¼‰
        "scooter_count": int(count),
        "ts": datetime.now().isoformat(),
        "src_id": src_id or "",
    }
    # on_conflict è¨˜å¾—æ˜¯è¤‡åˆéµ
    supabase.table("current_status").upsert(
        payload,
        on_conflict="route_key,area_id"
    ).execute()


def extract_group_from_rect_name(name: str) -> str:
    """
    å¾å­æ ¼åæŠ“è‹±æ–‡å­—æ¯é–‹é ­ï¼Œä¾‹å¦‚ 'C01' -> 'C'ã€'H' -> 'H'
    """
    if not name:
        return ""
    m = re.match(r"^[A-Za-z]+", name.strip())
    return m.group(0).upper() if m else ""

def extract_group_from_location(loc: str) -> str:
    if not loc:
        return ""
    s = loc.strip()
    # å…ˆæŠ“ã€Œé–‹é ­çš„è‹±æ–‡å­—æ¯ã€ï¼ˆC01 â†’ Cã€H02 â†’ Hï¼‰
    m = re.match(r"[A-Za-z]+", s)
    if m:
        return m.group(0).upper()
    # æŠ“çµå°¾çš„è‹±æ–‡å­—æ¯ï¼ˆâ€¦åœè»Šæ ¼D â†’ Dï¼‰
    m = re.search(r"([A-Za-z]+)$", s)
    return m.group(1).upper() if m else ""

def split_inferred_area(s: str):
    """
    'ib_A01' -> ('IB', 'A01', 'A')
    'tr_C02' -> ('TR', 'C02', 'C')
    å…¶é¤˜æ ¼å¼å› ('', '', '')
    """
    if not s:
        return "", "", ""
    s = s.strip().upper()
    m = re.match(r"^([A-Z]+)_([A-Z]+[0-9]+)$", s)
    if not m:
        return "", "", ""
    route_key = m.group(1)          # IB / TR
    area_id   = m.group(2)          # A01 / C02
    group_key = re.match(r"^[A-Z]+", area_id).group(0)  # A / C
    return route_key, area_id, group_key

# ----------------- JSON ç”¢å‡º -----------------
def generate_json_for_location(inferred_area: str):
    """ç”¢ç”Ÿå‰ç«¯å¯ç”¨ JSONï¼Œç´…é»æ²¿ä¸­å¿ƒç·šæ•´é½ŠåŒ–ï¼Œå«ç¶“ç·¯åº¦"""
    # 1) è§£æ inferred_area -> route_key / area_id / group_key
    route_key, area_id, default_group = split_inferred_area(inferred_area)

    # 2) æœ€æ–°åœ–ç‰‡ï¼ˆç”¨ inferred_area æŸ¥ï¼‰
    uploads = (
        supabase.table("image_uploads")
        .select("filename", "created_at")
        .eq("inferred_area", inferred_area)
        .order("created_at", desc=True)
        .limit(1)
        .execute()
    )
    if not uploads.data:
        print(f" {inferred_area} æ²’æœ‰ä»»ä½•åœ–ç‰‡ä¸Šå‚³ç´€éŒ„")
        return
    latest_filename = uploads.data[0]["filename"]

    # 3) è®€åŒä¸€å¼µåœ–çš„ motor_records
    records = (
        supabase.table("motor_records")
        .select("*")
        .eq("image_filename", latest_filename)
        .execute()
        .data
    )
    if not records:
        print(f"âš ï¸ {inferred_area} æ²’æœ‰ motor_records")
        return

    # 4) è®€å–åº•åœ–è¨­å®šï¼šroute_key + area_idï¼ˆå¤§å°å¯«ä¸æ•æ„Ÿï¼Œå« fallbackï¼‰
    rk = (route_key or "").strip().lower()   # DB å¯èƒ½å­˜ ib/trï¼ˆå°å¯«ï¼‰
    aid = (area_id or "").strip().upper()    # area ä¸€å¾‹ç”¨å¤§å¯«æ¯”å°

    # 4-1ï¼šå„ªå…ˆé›™éµï¼ˆroute_key ä¸åˆ†å¤§å°å¯«ï¼‰
    res = (
        supabase.table("base_configs")
        .select("*")
        .ilike("route_key", rk if rk else "%")
        .eq("area_id", aid)
        .limit(1)
        .execute()
    )
    box_data = res.data

    # 4-2ï¼šfallbackï¼šåªç”¨ area_idï¼ˆç›¸å®¹æ²’å¡« route_key çš„èˆŠè³‡æ–™ï¼‰
    if not box_data:
        res = (
            supabase.table("base_configs")
            .select("*")
            .eq("area_id", aid)
            .limit(1)
            .execute()
        )
        box_data = res.data

    # 4-3ï¼šfallbackï¼šæœ‰äººæŠŠ area_id å­˜æˆ 'ib_H01' é€™ç¨® â†’ æ¨¡ç³Šæ¯”å°
    if not box_data:
        res = (
            supabase.table("base_configs")
            .select("*")
            .ilike("area_id", f"%{aid}")
            .limit(1)
            .execute()
        )
        box_data = res.data

    if not box_data:
        print(f"âš ï¸ æ‰¾ä¸åˆ°åº•åœ–ï¼šroute_key='{rk}', area_id='{aid}'")
        try:
            cand_area = supabase.table("base_configs").select("route_key,area_id")\
                .ilike("area_id", f"%{aid}%").execute().data
            cand_route = supabase.table("base_configs").select("route_key,area_id")\
                .ilike("route_key", rk).execute().data if rk else []
            print("  â€¢ å¯èƒ½çš„ area å€™é¸ï¼š", cand_area)
            print("  â€¢ å¯èƒ½çš„ route å€™é¸ï¼š", cand_route)
        except Exception:
            pass
        return

    cfg = box_data[0]
    coords = json.loads(cfg["coords"])
    # å°‡è—æ¡†è½‰ç‚ºåƒç´ åº§æ¨™
    box_points = np.array([
        [
            (c["lng"] - cfg["lng_min"]) / (cfg["lng_max"] - cfg["lng_min"]) * cfg["img_width"],
            (cfg["lat_max"] - c["lat"]) / (cfg["lat_max"] - cfg["lat_min"]) * cfg["img_height"],
        ]
        for c in coords
    ], dtype=float)
    box_points = reorder_box_points(box_points)

    # 5) çµ„ markersï¼ˆåªåŠ å…¥å¯ç®—å‡º lat/lng çš„é»ï¼Œç¢ºä¿ points èˆ‡ markers å°é½Šï¼‰
    points, markers = [], []
    for item in records:
        x_px = item.get("real_x"); y_px = item.get("real_y")
        if x_px is None or y_px is None:
            continue
        if not isinstance(x_px, (int, float)) or not isinstance(y_px, (int, float)):
            continue

        lat, lng = pixel_to_latlng(x_px, y_px, cfg)
        if lat is None or lng is None:
            continue

        rect_name = item.get("rect_name") or item.get("subspot") or item.get("grid_name") or ""
        group_key = extract_group_from_rect_name(rect_name) or default_group  # å–ä¸åˆ°å°±ç”¨é è¨­(A/C...)

        points.append([x_px, y_px])
        markers.append({
            "motor_index": item["motor_index"],
            "plate_text": item["plate_text"],
            "pixel_x": int(x_px),
            "pixel_y": int(y_px),
            "lat": lat,
            "lng": lng,
            "location": inferred_area,            # ä¾‹å¦‚ 'IB_A01' æˆ– 'ib_A01'
            "image_filename": item["image_filename"],
            "group_key": group_key,               # A/B/C...
            "spot_group": group_key,              # åŒç¾©å‚™æ´
            "route_key": route_key,               # IB / TRï¼ˆåŸæ¨£å›å‚³ï¼Œçµ¦å‰ç«¯åƒè€ƒï¼‰
        })

    # 6) å°é½Šåˆ°ä¸­å¿ƒç·šï¼ˆzip é¿å…è¶Šç•Œï¼‰
    if points:
        aligned_pts = align_points_to_centerline(np.array(points, dtype=float), box_points)
        for m, (x, y) in zip(markers, aligned_pts):
            x = float(x); y = float(y)
            lat, lng = pixel_to_latlng(x, y, cfg)
            m["pixel_x"] = int(x); m["pixel_y"] = int(y)
            if lat is not None and lng is not None:
                m["lat"] = lat; m["lng"] = lng

    # 7) ç‹€æ…‹è¡¨ï¼šç”¨ route_key + area_id ç•¶ä¸»éµ
    latest_count = len(markers)
    if route_key and area_id:
        upsert_current_count(route_key, area_id, latest_count, src_id=latest_filename)
        print(f"ğŸŸ¢ current_status æ›´æ–°ï¼š({route_key}, {area_id}) = {latest_count}")
    else:
        print(f"âš ï¸ æœªæ›´æ–°ï¼šinferred_area='{inferred_area}' ç„¡æ³•è§£æåˆ° route_key/area_id")
        
    # 8) è¼¸å‡º JSONï¼ˆæª”åä¹Ÿç”¨ route+areaï¼‰
    out_dir = Path("map_outputs")
    out_dir.mkdir(exist_ok=True)  # æ²’æœ‰è³‡æ–™å¤¾å°±è‡ªå‹•å»ºç«‹
    out_path = out_dir / f"map_output_{route_key}_{area_id}.json"

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(markers, f, ensure_ascii=False, indent=2)

    print(f" {route_key}_{area_id}: å·²è¼¸å‡º {out_path} (å«ç¶“ç·¯åº¦ï¼Œæ²¿ä¸­å¿ƒç·šæ•´é½ŠåŒ–)")

def resolve_base_config_dir(inferred_area_value: str) -> str:
    """
    inferred_area_value ä¾‹: 'ib_H01' / 'tr_A02'
    å„ªå…ˆæ‰¾ base_config_<ROUTE>_<AREA>ï¼›æ‰¾ä¸åˆ°å°±é€€å› base_config_<AREA>
    """
    parts = (inferred_area_value or "").split("_", 1)
    if len(parts) != 2:
        # ä¸åˆæ³•å°±ç›´æ¥ç”¨åŸå­—ä¸²ï¼ˆé¿å…å´©ï¼‰
        return f"base_config_{inferred_area_value}"
    route_key, area_id = parts[0].upper(), parts[1].upper()
    p1 = f"base_config_{route_key}_{area_id}"
    p2 = f"base_config_{area_id}"
    return p1 if os.path.isdir(p1) else p2

if __name__ == "__main__":
    images = get_unprocessed_images_raw()
    if not images:
        print(" æ²’æœ‰æ–°çš„åœ–ç‰‡è¦è™•ç†")
    else:
        # ========= ç¬¬ 1 éšæ®µï¼šç‚ºæ‰€æœ‰æœªè™•ç†å½±åƒæ¨è«– inferred_area =========
        prepared = []  # æš«å­˜ï¼šæ¯å¼µåœ–çš„åŸºæœ¬è³‡è¨Š + æ¨è«–çµæœ
        for img in images:
            filename = img["filename"]
            image_id = img["id"]
            location = (img.get("location") or "").strip()  # è·¯æ®µï¼šib / tr

            # ä¸‹è¼‰å½±åƒï¼ˆè‹¥å·²å­˜åœ¨æœƒè¦†è“‹/è¤‡å¯«ï¼ŒOKï¼‰
            downloaded_path = download_image(filename)
            if downloaded_path is None:
                mark_as_processed(image_id)
                continue

            # åªåœ¨è©²ã€Œlocation çš„åº•åœ–åº«ã€ä¸­æ¨è«–ç´”å€ä»£è™Ÿï¼ˆA01/B01/...ï¼‰
            area_id = infer_area_by_kp(downloaded_path, str(BASE_IMAGES_ROOT), location)
            print(f"\nè™•ç†åœ–ç‰‡: {filename} @ {location}")
            print(f"ğŸ“ æ¨è«–åˆ°çš„å€åŸŸä»£è™Ÿ(area_id): {area_id}")

            inferred_area_value = f"{location}_{area_id}" if area_id else None

            # å¯«å› DBï¼šimage_uploads.inferred_area
            supabase.table("image_uploads")\
                .update({"inferred_area": inferred_area_value})\
                .eq("id", image_id)\
                .execute()

            if not area_id:
                print("âŒ ç„¡æ³•æ¨è«–å€åŸŸï¼ˆarea_id ç‚ºç©ºï¼‰ï¼Œå…ˆæ¨™è¨˜ processed è·³éæ­¤åœ–")
                mark_as_processed(image_id)
                continue

            prepared.append({
                "id": image_id,
                "filename": filename,
                "created_at": img.get("created_at", ""),
                "inferred_area": inferred_area_value,   # ä¾‹å¦‚ ib_H01
            })

        if not prepared:
            print(" æ²’æœ‰å®Œæˆå€åŸŸåˆ¤å®šçš„åœ–ç‰‡å¯è™•ç†")
            print("\nâœ… æ‰€æœ‰åœ°å€è™•ç†å®Œæˆï¼")
            sys.exit(0)

        # ========= ç¬¬ 2 éšæ®µï¼šä¾ inferred_area åªæŒ‘æœ€æ–°ä¸€å¼µåšå¾ŒçºŒè™•ç† =========
        latest_by_area = {}
        for row in prepared:
            area = (row.get("inferred_area") or "").strip()
            if not area:
                continue
            ts = row.get("created_at", "")
            if area not in latest_by_area or ts > latest_by_area[area].get("created_at", ""):
                latest_by_area[area] = row

        targets = list(latest_by_area.values())
        for tgt in targets:
            image_id = tgt["id"]
            filename = tgt["filename"]
            inferred_area_value = tgt["inferred_area"]  # â† å€éµï¼šib_H01 / tr_A02
            print(f"\nâ–¶ï¸ é–‹å§‹è™•ç†ï¼ˆæ¯å€æœ€æ–°ï¼‰: {filename} @ {inferred_area_value}")

            # æª”æ¡ˆä¸€å®šåœ¨ downloads/ï¼ˆä¸Šé¢ç¬¬ä¸€éšæ®µå·²ä¸‹è¼‰éï¼‰ï¼Œé€™é‚Šå†æ‹¿ä¸€æ¬¡è·¯å¾‘æ¯”è¼ƒç›´è¦º
            downloaded_path = os.path.join(DOWNLOAD_DIR, filename)

            # --- OCR ---
            ocr_json_path = os.path.abspath(os.path.join(DOWNLOAD_DIR, f"{filename}_ocr.json"))
            subprocess.run([
                "conda", "run", "-n", "ocr_env", "python", r"E:\ParkSavvy\mark\ocr.py",
                downloaded_path, ocr_json_path
            ], check=False)

            # --- YOLO + Homography ---
            base_config_dir = resolve_base_config_dir(inferred_area_value)

            # è·‘ä¹‹å‰åˆªèˆŠ resultï¼Œé¿å…èª¤ç”¨
            result_json_path = downloaded_path + "_result.json"
            try:
                if os.path.exists(result_json_path):
                    os.remove(result_json_path)
            except Exception:
                pass

            subprocess.run([
                "conda", "run", "-n", "yolo_paddle", "python", r"E:\ParkSavvy\mark\based_mark.py",
                downloaded_path, base_config_dir, ocr_json_path
            ], check=False)

            # --- ä¸Šå‚³ motor_recordsï¼ˆlocation= inferred_areaï¼‰ ---
            if os.path.exists(result_json_path):
                upload_motor_records(result_json_path, inferred_area_value, filename)
            else:
                print("âŒ based_mark åŸ·è¡Œå¤±æ•—ï¼Œæœªç”¢ç”Ÿ result.jsonï¼Œè·³éæ­¤åœ–")
                mark_as_processed(image_id)
                continue

            # --- ç”¢åœ°åœ– JSONï¼ˆç”¨ inferred_areaï¼‰ ---
            generate_json_for_location(inferred_area_value)

            # --- æ¨™è¨˜ processed ---
            mark_as_processed(image_id)
            # åŒä¸€å€çš„å…¶ä»–èˆŠåœ–ä¹Ÿä¸€ä½µæ¨™ processedï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
            supabase.table("image_uploads")\
                .update({"processed": True})\
                .eq("inferred_area", inferred_area_value)\
                .neq("id", image_id)\
                .execute()

    print("\nâœ… æ‰€æœ‰åœ°å€è™•ç†å®Œæˆï¼")